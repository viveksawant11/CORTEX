{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'cortex'\n",
    "\n",
    "# Get keys from the environment\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if langchain_api_key:\n",
    "    os.environ['LANGCHAIN_API_KEY'] = langchain_api_key\n",
    "else:\n",
    "    raise ValueError(\"LANGCHAIN_API_KEY is not set in the environment.\")\n",
    "\n",
    "if groq_api_key:\n",
    "    os.environ['GROQ_API_KEY'] = groq_api_key\n",
    "else:\n",
    "    raise ValueError(\"GROQ_API_KEY is not set in the environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 5 - MULTI QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Set USER_AGENT before making requests\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split - Chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Embed\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "# Initialize HuggingFaceEmbeddings\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Index with FAISS\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=hf_embeddings)\n",
    "\n",
    "# Retrieve\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Query Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are five alternative versions of the original question:\n",
      "What are the advantages of LangChain in natural language processing applications?\n",
      "How does LangChain improve the performance of NLP models?\n",
      "What are the key benefits of integrating LangChain into NLP pipelines?\n",
      "Can LangChain enhance the efficiency of NLP workflows can benefit from LangChain?\n",
      "What are the use cases where LangChain excels in NLP tasks?\n",
      "These alternative questions can help retrieve relevant documents from a vector database by providing different perspectives on the original question, which can overcome some of the limitations of similarity search.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Disable LangSmith tracing to prevent API errors\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\" \n",
    "\n",
    "# Define the template for generating multiple perspectives on the user's question\n",
    "template = \"\"\"\n",
    "You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "\n",
    "Provide these alternative questions separated by newlines.\n",
    "\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template using ChatPromptTemplate\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-70b-8192\" \n",
    ")\n",
    "\n",
    "# Define the pipeline for generating alternative questions\n",
    "generate_queries = (\n",
    "    prompt_perspectives     # Use the prompt template\n",
    "    | llm                   # Use the ChatGroq model with zero temperature (deterministic responses)\n",
    "    | StrOutputParser()     # Parse the output into a clean format (a single string with questions separated by newlines)\n",
    "    | (lambda x: \"\\n\".join([q.strip() for q in x.split(\"\\n\") if q.strip()]))  # Clean and join questions with newlines\n",
    ")\n",
    "\n",
    "# Example: Use the pipeline to generate alternative questions for a given user question\n",
    "user_question = \"What are the benefits of using LangChain for NLP tasks?\"\n",
    "alternative_questions = generate_queries.invoke({\"question\": user_question})\n",
    "\n",
    "# Print the generated alternative questions\n",
    "print(alternative_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/7gm4yt417px42myyczdlqf1h0000gn/T/ipykernel_48433/2784938305.py:10: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, task decomposition for LLM agents is not explicitly defined. But we can understand the concept of task decomposition in general.\n",
      "Task decomposition is the process of breaking down a complex task into smaller, more manageable sub-tasks. This is often necessary for LLM agents because they can only handle a limited amount of context and may not be able to process a large, complex task in a single step.\n",
      "In the context of LLM agents, task decomposition might involve identifying the key steps required to complete a task, and then breaking those steps down into smaller sub-tasks that can be executed by the agent. This might involve identifying the inputs required for each sub-task, the agent needs to take, and the outputs it needs to produce.\n",
      "For example, in the context of building a Super Mario game in Python, the task decomposition might involve breaking down the task into sub-tasks such as:\n",
      "* Designing the game's architecture (MVC components)\n",
      "* Implementing keyboard control\n",
      "* Creating game objects (characters, enemies, etc.)\n",
      "* Developing the game's logic (level design, gameplay mechanics, etc.)\n",
      "Each of these sub-tasks can then be executed by the LLM agent, which can process the inputs and produce the required outputs.\n",
      "It's worth noting that the context provided does not explicitly define task decomposition for LLM agents, but it does provide examples of how tasks can be broken down into smaller sub-tasks, which is a key aspect of task decomposition.\n"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG template\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0)\n",
    "\n",
    "# Define the final RAG chain\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")}  # Ensure the mapping is correct\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: \"\\n\".join([q.strip() for q in x.split(\"\\n\") if q.strip()]))\n",
    ")\n",
    "\n",
    "# Define the question\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "# Invoke the chain with the question and get the result\n",
    "result = final_rag_chain.invoke({\"question\": question})\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 6 - RAG-FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"\n",
    "You are a helpful assistant tasked with generating multiple search queries based on a single input query. \n",
    "The goal is to create queries that are semantically related to the input while capturing different aspects of the topic.\n",
    "\n",
    "Input Query: {question}\n",
    "\n",
    "Output: Provide exactly 4 related search queries, each on a new line.\n",
    "\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Define the query generation pipeline\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatGroq(model=\"llama3-70b-8192\", temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: [\n",
    "        query.strip().lstrip(\"1234567890. \")  # Remove numbering and extra spaces\n",
    "        for query in x.split(\"\\n\") if query.strip()  # Remove empty lines\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieved documents: 9\n"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"Applies Reciprocal Rank Fusion (RRF) to combine ranked document lists.\"\"\"\n",
    "    fused_scores = {}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)  # Serialize document for use as dictionary key\n",
    "            fused_scores[doc_str] = fused_scores.get(doc_str, 0) + 1 / (rank + k)\n",
    "\n",
    "    # Sort by score in descending order and deserialize documents\n",
    "    return [\n",
    "        (loads(doc), score) for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "# RAG Fusion Retrieval Chain\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "\n",
    "# Invoke the chain with a question\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "# Output the length of the fused document list\n",
    "print(f\"Number of retrieved documents: {len(docs)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not directly answer the question about the impact of AI on healthcare. However, it does mention some AI-related concepts, like LLMs (Large Language Models), tool-augmented LLMs, which could be applied to healthcare.\n",
      "One of the mentioned papers, \"ChemCrow: Augmenting large-language models with chemistry tools\" (Bran et al., 2023), explores the use of LLMs in chemistry, which could have implications for healthcare. Another paper, \"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace\" (Shen et al., 2023), discusses the use of ChatGPT as a task planner to select models available in the HuggingFace platform, which could also be applied to healthcare.\n",
      "While the provided context does not directly answer the question, it suggests that AI, particularly LLMs, has the potential to be applied to healthcare and could have a significant impact.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "# Define your context and question prompt\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Assuming retrieval_chain_rag_fusion is defined earlier\n",
    "# You need to pass the context from your retrieval chain and the question as the input\n",
    "\n",
    "# Define the final RAG chain\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | ChatGroq(model=\"llama3-70b-8192\", temperature=0)  \n",
    "    | StrOutputParser()\n",
    "    | (lambda x: \"\\n\".join([line.strip() for line in x.split(\"\\n\") if line.strip()]))\n",
    ")\n",
    "\n",
    "# Make sure the input is correctly structured\n",
    "question = \"What is the impact of AI on healthcare?\"\n",
    "\n",
    "# Now invoke the chain with the correctly formatted input\n",
    "response = final_rag_chain.invoke({\"question\": question})\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 7 - DECOMPOSITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition Template\n",
    "template = \"\"\"\n",
    "You are a helpful assistant tasked with breaking down a complex input question into smaller, focused sub-questions. \n",
    "Each sub-question should address a specific aspect of the main question, allowing them to be answered in isolation.\n",
    "\n",
    "Input: {question}\n",
    "\n",
    "Your goal:\n",
    "- Decompose the input question into exactly 3 relevant and concise sub-questions.\n",
    "- Ensure each sub-question is clear, non-overlapping, and focused on a distinct aspect of the input.\n",
    "\n",
    "Output:\n",
    "1. [First sub-question]\n",
    "2. [Second sub-question]\n",
    "3. [Third sub-question]\n",
    "\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the 3 sub-questions that break down the complex input question:\n",
      "1. What are the key architectural components of an LLM-powered autonomous agent system, such as the role of the LLM, perception, and action modules?\n",
      "2. What are the primary functions and responsibilities of the LLM within the autonomous agent system, including tasks such as reasoning, planning, and decision-making?\n",
      "3. How does the autonomous agent system integrate and leverage external data sources, such as sensors, APIs, and knowledge graphs, to inform its decision-making and action-taking processes?\n",
      "These sub-questions address distinct aspects of the main question, allowing them to be answered independently while still providing a comprehensive understanding of the main components of an LLM-powered autonomous agent system.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0)\n",
    "\n",
    "# Chain for generating sub-questions via decomposition\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition  # The prompt template\n",
    "    | llm  # Language model to process the prompt\n",
    "    | StrOutputParser()  # Parses the LLM output to a string\n",
    "    | (lambda x: [q.strip() for q in x.split(\"\\n\") if q.strip()])  # Clean and split into a list of sub-questions\n",
    ")\n",
    "\n",
    "# Input Question\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "\n",
    "# Run the Chain\n",
    "try:\n",
    "    questions = generate_queries_decomposition.invoke({\"question\": question})\n",
    "    # Print the generated sub-questions\n",
    "    print(\"\\n\".join(questions))\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful assistant tasked with answering a given question using the provided background question-answer pairs and additional context.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Background Question-Answer Pairs:\n",
    "{q_a_pairs}\n",
    "\n",
    "Additional Context:\n",
    "{context}\n",
    "\n",
    "Using the above information, provide a clear and concise answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Question: Here are the 3 sub-questions that break down the complex input question:\n",
      "Answer: Based on the provided background question-answer pairs and additional context, the 3 sub-questions that break down the complex input question are:\n",
      "\n",
      "1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\n",
      "2. Details about the MVC components (e.g. which components are in each file)\n",
      "3. Keyboard control implementation (e.g. which keys to use, how to handle input)\n",
      "\n",
      "These sub-questions are derived from the conversation samples provided, where the assistant clarifies the user's input and breaks it down into specific areas that need further clarification.\n",
      "---\n",
      "Question: 1. What are the key architectural components of an LLM-powered autonomous agent system, such as the role of the LLM, perception, and action modules?\n",
      "Answer: Based on the provided background question-answer pairs and additional context, the key architectural components of an LLM-powered autonomous agent system are:\n",
      "\n",
      "1. **LLM (Large Language Model)**: The brain of the autonomous agent, responsible for general problem-solving, planning, and decision-making. It functions as a router to route inquiries to the best suitable expert module** (in a MRKL system).\n",
      "\n",
      "2. **Planning Module**: Responsible for task decomposition, subgoal and decomposition, reflection, and refinement. It involves techniques like Chain of Thought (CoT) and Tree of Thoughts to break down complex tasks into manageable steps.\n",
      "\n",
      "3. **Perception Module**: Not explicitly mentioned in the provided context, but it can be inferred that it would be responsible for processing and interpreting sensory data from the environment.\n",
      "\n",
      "4. **Action Module**: Also not explicitly mentioned, but it can be inferred that it would be responsible for executing the planned actions in the environment.\n",
      "\n",
      "5. **Memory Module**: Responsible for storing and retrieving information, enabling the agent to learn from past experiences and refine its actions.\n",
      "\n",
      "6. **Expert Modules** (in a MRKL system): These can be neural (e.g., deep learning models) or symbolic (e.g., math calculator, currency converter, weather API). They are responsible for providing specific knowledge and skills to the agent.\n",
      "\n",
      "These components work together to enable the LLM-powered agent to perceive its environment, plan and make decisions, and take actions to achieve its goals.\n",
      "---\n",
      "Question: 2. What are the primary functions and responsibilities of the LLM within the autonomous agent system, including tasks such as reasoning, planning, and decision-making?\n",
      "Answer: Based on the provided background question-answer pairs and additional context, the primary functions and responsibilities of the LLM (Large Language Model) within the LLM-powered autonomous agent system are:\n",
      "\n",
      "* **General problem-solving**: The LLM functions as a router to route inquiries to the best suitable expert module (in a MRKL system).\n",
      "* **Reasoning**: The LLM is involved in reasoning and thinking step-by-step, utilizing techniques like Chain of Thought (CoT) and Tree of Thoughts to break down complex tasks into manageable steps.\n",
      "* **Planning**: The LLM is responsible for task decomposition, subgoal decomposition, reflection, and refinement. It involves techniques like Chain of Thought (CoT) and Tree of Thoughts to break down complex tasks into manageable steps.\n",
      "* **Decision-making**: The LLM makes decisions by evaluating states and selecting the best course of action, which can be done through various methods such as BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "In summary, the LLM is the brain of the agent, responsible for general problem-solving, reasoning, planning, and decision-making, which enables the agent to perceive its environment, plan and make decisions, and take actions to achieve its goals.\n",
      "---\n",
      "Question: 3. How does the autonomous agent system integrate and leverage external data sources, such as sensors, APIs, and knowledge graphs, to inform its decision-making and action-taking processes?\n",
      "Answer: Based on the provided background question-answer pairs and additional context, the autonomous agent system integrates and leverages external data sources, such as sensors, APIs, and knowledge graphs, to inform its decision-making and action-taking processes through the following components:\n",
      "\n",
      "1. **Perception Module**: This module processes and interprets sensory data from the environment, which can include data from sensors, APIs, and knowledge graphs. The perception module provides the necessary information for the planning module to make informed decisions.\n",
      "\n",
      "2. **Planning Module**: This module uses techniques like Chain of Thought (CoT) and Tree of Thoughts to break down complex tasks into manageable steps. It involves task decomposition, subgoal decomposition, and refinement. The planning module takes into account the information provided by the perception module and expert modules to plan and make decisions.\n",
      "\n",
      "3. **Expert Modules**: These modules provide specific knowledge and skills to the agent. They can be neural (e.g., deep learning models) or symbolic (e.g., math calculator, currency converter, weather API). The expert modules can leverage external data sources, such as knowledge graphs, to inform the agent's decision-making and action-taking processes.\n",
      "\n",
      "4. **Memory Module**: This module stores and retrieves information, enabling the agent to learn from past experiences and refine its actions. The memory module can store information from external data sources, such as knowledge graphs, to inform the agent's decision-making and action-taking processes.\n",
      "\n",
      "5. **LLM (Large Language Model)**: The LLM functions as a router to route inquiries to the best suitable expert module. It is involved in general problem-solving, reasoning, planning, and decision-making. The LLM can leverage external data sources, such as APIs and knowledge graphs, to inform its decision-making and action-taking processes.\n",
      "\n",
      "In summary, the autonomous agent system integrates and leverages external data sources, such as sensors, APIs, and knowledge graphs, through its perception module, planning module, expert modules, memory module, and LLM. These components work together to enable the agent to perceive its environment, plan and make decisions, and take actions to achieve its goals.\n",
      "---\n",
      "Question: These sub-questions address distinct aspects of the main question, allowing them to be answered independently while still providing a comprehensive understanding of the main components of an LLM-powered autonomous agent system.\n",
      "Answer: The three sub-questions that break down the complex input question are:\n",
      "\n",
      "1. What are the key architectural components of an LLM-powered autonomous agent system, such as the role of the LLM, perception, and action modules?\n",
      "2. What are the primary functions and responsibilities of the LLM within the autonomous agent system, including tasks such as reasoning, planning, and decision-making?\n",
      "3. How does the autonomous agent system integrate and leverage external data sources, such as sensors, APIs, and knowledge graphs, to inform its decision-making and action-taking processes?\n",
      "\n",
      "These sub-questions address distinct aspects of the main question, allowing them to be answered independently while still providing a comprehensive understanding of the main components of an LLM-powered autonomous agent system.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format a question and answer pair as a string.\"\"\"\n",
    "    return f\"Question: {question}\\nAnswer: {answer}\\n\"\n",
    "\n",
    "# Initialize the language model (LLM)\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0)\n",
    "\n",
    "# Initialize Q&A pairs as an empty string\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "# Loop through the list of questions and process each one\n",
    "for q in questions:\n",
    "    # Define the RAG chain for retrieving and answering\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever,  # Retrieve context for the current question\n",
    "            \"question\": itemgetter(\"question\"),             # Extract the current question\n",
    "            \"q_a_pairs\": itemgetter(\"q_a_pairs\")            # Include existing Q&A pairs\n",
    "        }\n",
    "        | decomposition_prompt  # Apply the decomposition prompt\n",
    "        | llm                   # Use the LLM to generate an answer\n",
    "        | StrOutputParser()     # Parse the output into a clean string\n",
    "    )\n",
    "\n",
    "    # Generate an answer for the current question\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "\n",
    "    # Format the question and answer pair\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "\n",
    "    # Append the new Q&A pair to the existing pairs\n",
    "    q_a_pairs += f\"\\n---\\n{q_a_pair.strip()}\"\n",
    "\n",
    "# Print the final Q&A pairs (optional)\n",
    "print(q_a_pairs.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer Individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Pull the RAG prompt template from the hub\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    \"\"\"\n",
    "    Perform RAG (Retrieve and Generate) for each sub-question.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The main input question.\n",
    "        prompt_rag: The RAG prompt template.\n",
    "        sub_question_generator_chain: A chain that generates sub-questions.\n",
    "        retriever: A retriever object to fetch relevant documents.\n",
    "        llm: The language model for answering questions.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A list of answers and the corresponding sub-questions.\n",
    "    \"\"\"\n",
    "    # Generate sub-questions from the input question\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\": question})\n",
    "\n",
    "    # Initialize a list to store answers for each sub-question\n",
    "    rag_results = []\n",
    "\n",
    "    # Process each sub-question\n",
    "    for sub_question in sub_questions:\n",
    "        # Retrieve documents relevant to the sub-question\n",
    "        retrieved_docs = retriever.invoke(sub_question)\n",
    "\n",
    "        # Generate an answer using the RAG prompt, LLM, and retrieved documents\n",
    "        answer = (\n",
    "            prompt_rag  # Apply the RAG prompt\n",
    "            | llm       # Pass it through the language model\n",
    "            | StrOutputParser()  # Parse the LLM output into a clean string\n",
    "        ).invoke({\"context\": retrieved_docs, \"question\": sub_question})\n",
    "\n",
    "        # Append the generated answer to the results list\n",
    "        rag_results.append(answer)\n",
    "\n",
    "    return rag_results, sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided Q&A pairs, here is a clear and concise answer to the question:\n",
      "\n",
      "The main components of an LLM-powered autonomous agent system include:\n",
      "\n",
      "1. The LLM (brain), which breaks down large tasks into manageable subgoals, enables efficient handling of complex tasks, and does self-reflection and refinement over past actions.\n",
      "3. A planning module, which enables subgoal decomposition and reflection.\n",
      "4. Action modules, which execute tasks.\n",
      "5. A retrieval model, which surfaces context to inform the agent's behavior based on relevance, recency, and importance.\n",
      "6. Inter-agent communication, which can trigger new natural language statements.\n",
      "7. Environment information present in a tree structure, which informs planning and reacting.\n",
      "\n",
      "These components work together to enable the autonomous agent system to integrate and leverage external data sources, such as sensors, APIs, and knowledge graphs, to inform its decision-making and action-taking processes.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"\n",
    "    Format a list of questions and their corresponding answers into a structured string.\n",
    "\n",
    "    Args:\n",
    "        questions (list): List of questions.\n",
    "        answers (list): List of answers corresponding to the questions.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted string of Q&A pairs.\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Question {i}: {q}\\nAnswer {i}: {a}\" for i, (q, a) in enumerate(zip(questions, answers), start=1)\n",
    "    ).strip()\n",
    "\n",
    "# Format the Q&A pairs to create the context\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Define the prompt template for synthesizing an answer\n",
    "template = \"\"\"\n",
    "Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize a clear and concise answer to the following question:\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define the RAG chain\n",
    "final_rag_chain = (\n",
    "    prompt  # Use the prompt template\n",
    "    | llm  # Pass the input through the language model\n",
    "    | StrOutputParser()  # Parse the output into a clean string\n",
    ")\n",
    "\n",
    "# Invoke the chain with the given context and question\n",
    "result = final_rag_chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "# Output the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 8 - STEP BACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# Define few-shot examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"What can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel was born in what country?\",\n",
    "        \"output\": \"What is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Transform examples into example messages using a message prompt template\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a few-shot prompt template with the example prompt and examples\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Combine the system message, few-shot examples, and user question into the final prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert in world knowledge. Your task is to reframe a specific question into a more generic, step-back question that is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Add few-shot examples\n",
    "        few_shot_prompt,\n",
    "        # Add the user question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do artificial intelligence systems break down complex tasks?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | ChatGroq(model=\"llama3-70b-8192\", temperature=0) | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition for LLM (Large Language Model) agents refers to the process of breaking down a complex task into smaller, more manageable sub-tasks. This is a crucial component of planning in LLM-powered autonomous agent systems.\n",
      "\n",
      "There are several ways to perform task decomposition, including:\n",
      "\n",
      "1. **Simple prompting**: The LLM can be instructed to think step-by-step using prompts like \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\"\n",
      "2. **Task-specific instructions**: The LLM can be provided with specific instructions for a task, such as \"Write a story outline\" for writing a novel.\n",
      "3. **Human inputs**: Task decomposition can also be done with human inputs, where humans provide guidance on how to break down a complex task into smaller steps.\n",
      "\n",
      "Additionally, there are techniques like **Chain of Thought (CoT)** and **Tree of Thoughts** that can be used to enhance task decomposition. CoT involves instructing the model to \"think step by step\" and generate multiple manageable sub-tasks. Tree of Thoughts explores multiple reasoning possibilities at each step, creating a tree structure.\n",
      "\n",
      "Task decomposition is essential in LLM-powered autonomous agents as it allows the agent to plan ahead, understand the task's requirements, and execute the task into smaller, more manageable steps.\n"
     ]
    }
   ],
   "source": [
    "# Define the response prompt template\n",
    "response_prompt_template = \"\"\"\n",
    "You are an expert in world knowledge. I will ask you a question. Your response should be comprehensive and aligned with the provided contexts if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# Normal Context:\n",
    "{normal_context}\n",
    "\n",
    "# Step-Back Context:\n",
    "{step_back_context}\n",
    "\n",
    "# Original Question:\n",
    "{question}\n",
    "\n",
    "# Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create the response prompt\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "# Define the chain for context retrieval and response generation\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve normal context using the original question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve step-back context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass through the original question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt  # Generate a prompt with the retrieved context and question\n",
    "    | ChatGroq(model=\"llama3-70b-8192\", temperature=0)  # Generate the response using the LLM\n",
    "    | StrOutputParser()  # Parse the response into a clean string\n",
    ")\n",
    "\n",
    "# Invoke the chain with the input question\n",
    "result = chain.invoke({\"question\": question})\n",
    "\n",
    "# Print the generated response\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 9 - HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a scientific paper passage that answers the question:\n",
      "\n",
      "**Task Decomposition for Large Language Models (LLMs) Agents**\n",
      "\n",
      "In the realm of artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools for tackling complex tasks, such as natural language processing, text generation, and dialogue systems. However, as the complexity of these tasks continues to grow, it has become increasingly important to develop strategies for decomposing them into more manageable sub-tasks. This process, known as task decomposition, is a crucial step in enabling LLM agents to reason about and execute complex tasks in a more efficient and effective manner.\n",
      "\n",
      "In the context of LLM agents, task decomposition involves identifying the constituent sub-tasks, or \"micro-tasks,\" that comprise a larger task. These micro-tasks are typically smaller, more focused, and more easily executable by the LLM agent. For example, in a task such as \"write a short story about a character who learns to play the guitar,\" the micro-tasks might include generating a character profile, creating a plot outline, and writing descriptive paragraphs about the character's guitar-playing experiences. By breaking down the task into these smaller components, the LLM agent can more easily reason about the task, allocate resources, and execute each micro-task in a more focused and efficient way.\n",
      "\n",
      "Task decomposition for LLM agents offers several benefits, including improved task execution efficiency, enhanced reasoning capabilities, and increased flexibility in adapting to changing task requirements. By decomposing complex tasks into smaller, more manageable components, LLM agents can better allocate their computational resources, reduce the risk of errors, and improve overall task performance. As the complexity of tasks continues to grow, the development of effective task decomposition strategies will play a critical role in unlocking the full potential of LLM agents.\n",
      "\n",
      "I hope this passage meets your requirements!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Define the HyDE document generation template\n",
    "template = \"\"\"\n",
    "Please write a scientific paper passage to answer the following question:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Passage:\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt template using ChatPromptTemplate\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define the document generation chain\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde  # Use the HyDE prompt template\n",
    "    | ChatGroq(model=\"llama3-70b-8192\", temperature=0)  # Generate the document using the language model\n",
    "    | StrOutputParser()  # Parse the LLM output into a clean string\n",
    ")\n",
    "\n",
    "# Define the question\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "# Invoke the chain to generate the passage\n",
    "generated_passage = generate_docs_for_retrieval.invoke({\"question\": question})\n",
    "\n",
    "# Print the generated passage\n",
    "print(generated_passage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='0f4ac3ad-83c4-456d-8664-7fee2dd54dc2', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(id='09a42e0b-d21c-4556-9feb-30d63087f318', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.'),\n",
       " Document(id='cef84871-29ee-40c8-a0ad-485d700ab0d9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n",
       " Document(id='baa42ff9-6afa-4f38-a63f-266af67aff2c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Or\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retrieved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, task decomposition for LLM agents involves breaking down complex tasks into smaller, simpler steps. This can be achieved in three ways:\n",
      "\n",
      "1. Simple prompting, such as \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\"\n",
      "2. Using task-specific instructions, for example, \"Write a story outline.\" for writing a novel.\n",
      "3. With human inputs.\n",
      "\n",
      "Additionally, techniques like Chain of Thought (CoT) and Tree of Thoughts can be used to decompose tasks into smaller steps. CoT involves instructing the model to \"think step by step\" to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. Tree of Thoughts extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Define the RAG prompt template\n",
    "template = \"\"\"\n",
    "Answer the following question based on the provided context:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt from the template\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define the RAG chain\n",
    "final_rag_chain = (\n",
    "    prompt  # Generate the prompt using the template\n",
    "    | ChatGroq(model=\"llama3-70b-8192\", temperature=0)  # Use the LLM to process the prompt\n",
    "    | StrOutputParser()  # Parse the output into a clean string\n",
    ")\n",
    "\n",
    "# Invoke the RAG chain with the context & question\n",
    "result = final_rag_chain.invoke({\n",
    "    \"context\": retrieved_docs,  # The retrieved documents as context\n",
    "    \"question\": question  # The question to be answered\n",
    "})\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MP-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
