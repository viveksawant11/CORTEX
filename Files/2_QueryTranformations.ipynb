{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'cortex'\n",
    "\n",
    "# Get keys from the environment\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if langchain_api_key:\n",
    "    os.environ['LANGCHAIN_API_KEY'] = langchain_api_key\n",
    "else:\n",
    "    raise ValueError(\"LANGCHAIN_API_KEY is not set in the environment.\")\n",
    "\n",
    "if groq_api_key:\n",
    "    os.environ['GROQ_API_KEY'] = groq_api_key\n",
    "else:\n",
    "    raise ValueError(\"GROQ_API_KEY is not set in the environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 5 - MULTI QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Set USER_AGENT before making requests\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split - Chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Embed\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "# Initialize HuggingFaceEmbeddings\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Index with FAISS\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=hf_embeddings)\n",
    "\n",
    "# Retrieve\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Query Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are five alternative versions of the original question:\n",
      "What are the advantages of LangChain in natural language processing applications?\n",
      "How does LangChain improve the performance of NLP models?\n",
      "What are the key benefits of integrating LangChain into NLP pipelines?\n",
      "Can LangChain enhance the efficiency of NLP workflows can benefit from LangChain?\n",
      "What are the use cases where LangChain excels in NLP tasks?\n",
      "These alternative questions can help retrieve relevant documents from a vector database by providing different perspectives on the original question, which can overcome some of the limitations of similarity search.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Disable LangSmith tracing to prevent API errors\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\" \n",
    "\n",
    "# Define the template for generating multiple perspectives on the user's question\n",
    "template = \"\"\"\n",
    "You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "\n",
    "Provide these alternative questions separated by newlines.\n",
    "\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template using ChatPromptTemplate\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-70b-8192\" \n",
    ")\n",
    "\n",
    "# Define the pipeline for generating alternative questions\n",
    "generate_queries = (\n",
    "    prompt_perspectives     # Use the prompt template\n",
    "    | llm                   # Use the ChatGroq model with zero temperature (deterministic responses)\n",
    "    | StrOutputParser()     # Parse the output into a clean format (a single string with questions separated by newlines)\n",
    "    | (lambda x: \"\\n\".join([q.strip() for q in x.split(\"\\n\") if q.strip()]))  # Clean and join questions with newlines\n",
    ")\n",
    "\n",
    "# Example: Use the pipeline to generate alternative questions for a given user question\n",
    "user_question = \"What are the benefits of using LangChain for NLP tasks?\"\n",
    "alternative_questions = generate_queries.invoke({\"question\": user_question})\n",
    "\n",
    "# Print the generated alternative questions\n",
    "print(alternative_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/7gm4yt417px42myyczdlqf1h0000gn/T/ipykernel_55892/2784938305.py:10: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, task decomposition for LLM agents is not explicitly defined. However, we can infer some information about task decomposition from the context.\n",
      "In the context, we see that the AI assistant can parse user input into several tasks, and each task has a specific format with fields like \"task\", \"id\", \"dep\", and \"args\". The \"dep\" field denotes the id of the previous task that generates a new resource that the current task relies on.\n",
      "We also see that the task MUST be selected from a list of available tasks, and there is a logical relationship between tasks, with a specific order.\n",
      "Additionally, we see that the LLM distributes tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from, and task type-based filtration is needed due to the limited context length.\n",
      "Therefore, we can infer that task decomposition for LLM agents involves breaking down complex tasks into smaller, more manageable sub-tasks, and then distributing these sub-tasks to expert models or agents that can execute them. The LLM agent acts as a coordinator, parsing user input, decomposing tasks, and distributing them to the relevant models or agents, and ensuring that the tasks are executed in the correct order.\n"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG template\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0)\n",
    "\n",
    "# Define the final RAG chain\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")}  # Ensure the mapping is correct\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: \"\\n\".join([q.strip() for q in x.split(\"\\n\") if q.strip()]))\n",
    ")\n",
    "\n",
    "# Define the question\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "# Invoke the chain with the question and get the result\n",
    "result = final_rag_chain.invoke({\"question\": question})\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 6 - RAG-FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"\n",
    "You are a helpful assistant tasked with generating multiple search queries based on a single input query. \n",
    "The goal is to create queries that are semantically related to the input while capturing different aspects of the topic.\n",
    "\n",
    "Input Query: {question}\n",
    "\n",
    "Output: Provide exactly 4 related search queries, each on a new line.\n",
    "\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Define the query generation pipeline\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatGroq(model=\"llama3-70b-8192\", temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: [\n",
    "        query.strip().lstrip(\"1234567890. \")  # Remove numbering and extra spaces\n",
    "        for query in x.split(\"\\n\") if query.strip()  # Remove empty lines\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieved documents: 9\n"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"Applies Reciprocal Rank Fusion (RRF) to combine ranked document lists.\"\"\"\n",
    "    fused_scores = {}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)  # Serialize document for use as dictionary key\n",
    "            fused_scores[doc_str] = fused_scores.get(doc_str, 0) + 1 / (rank + k)\n",
    "\n",
    "    # Sort by score in descending order and deserialize documents\n",
    "    return [\n",
    "        (loads(doc), score) for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "# RAG Fusion Retrieval Chain\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "\n",
    "# Invoke the chain with a question\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "# Output the length of the fused document list\n",
    "print(f\"Number of retrieved documents: {len(docs)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not directly discuss the impact of AI on healthcare. However, it does mention ChemCrow, which is a tool-augmented LLM that has been used to develop a novel anticancer drug. This suggests that AI can be used to aid in scientific discovery and potentially improve healthcare outcomes. Additionally, the context mentions health data management as one of the APIs used in the API-Bank benchmark for evaluating tool-augmented LLMs. This implies that AI can be used to manage and analyze health data, which can lead to better healthcare decisions and outcomes.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "# Define your context and question prompt\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Assuming retrieval_chain_rag_fusion is defined earlier\n",
    "# You need to pass the context from your retrieval chain and the question as the input\n",
    "\n",
    "# Define the final RAG chain\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | ChatGroq(model=\"llama3-70b-8192\", temperature=0)  \n",
    "    | StrOutputParser()\n",
    "    | (lambda x: \"\\n\".join([line.strip() for line in x.split(\"\\n\") if line.strip()]))\n",
    ")\n",
    "\n",
    "# Make sure the input is correctly structured\n",
    "question = \"What is the impact of AI on healthcare?\"\n",
    "\n",
    "# Now invoke the chain with the correctly formatted input\n",
    "response = final_rag_chain.invoke({\"question\": question})\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 7 - DECOMPOSITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition Template\n",
    "template = \"\"\"\n",
    "You are a helpful assistant tasked with breaking down a complex input question into smaller, focused sub-questions. \n",
    "Each sub-question should address a specific aspect of the main question, allowing them to be answered in isolation.\n",
    "\n",
    "Input: {question}\n",
    "\n",
    "Your goal:\n",
    "- Decompose the input question into exactly 3 relevant and concise sub-questions.\n",
    "- Ensure each sub-question is clear, non-overlapping, and focused on a distinct aspect of the input.\n",
    "\n",
    "Output:\n",
    "1. [First sub-question]\n",
    "2. [Second sub-question]\n",
    "3. [Third sub-question]\n",
    "\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the 3 sub-questions that break down the complex input question:\n",
      "1. What are the key architectural components of an LLM-powered autonomous agent system, such as the role of the LLM, perception, and action modules?\n",
      "2. What are the primary functions and responsibilities of the LLM within the autonomous agent system, including tasks such as reasoning, planning, and decision-making?\n",
      "3. How does the autonomous agent system integrate and leverage external data sources, such as sensors, APIs, and knowledge graphs, to inform its decision-making and action-taking processes?\n",
      "These sub-questions address distinct aspects of the main question, allowing them to be answered independently while still providing a comprehensive understanding of the main components of an LLM-powered autonomous agent system.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0)\n",
    "\n",
    "# Chain for generating sub-questions via decomposition\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition  # The prompt template\n",
    "    | llm  # Language model to process the prompt\n",
    "    | StrOutputParser()  # Parses the LLM output to a string\n",
    "    | (lambda x: [q.strip() for q in x.split(\"\\n\") if q.strip()])  # Clean and split into a list of sub-questions\n",
    ")\n",
    "\n",
    "# Input Question\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "\n",
    "# Run the Chain\n",
    "try:\n",
    "    questions = generate_queries_decomposition.invoke({\"question\": question})\n",
    "    # Print the generated sub-questions\n",
    "    print(\"\\n\".join(questions))\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful assistant tasked with answering a given question using the provided background question-answer pairs and additional context.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Background Question-Answer Pairs:\n",
    "{q_a_pairs}\n",
    "\n",
    "Additional Context:\n",
    "{context}\n",
    "\n",
    "Using the above information, provide a clear and concise answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Question: Here are the 3 sub-questions that break down the complex input question:\n",
      "Answer: Based on the provided background question-answer pairs and additional context, the 3 sub-questions that break down the complex input question are:\n",
      "\n",
      "1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\n",
      "2. Details about the MVC components (e.g. which components are in each file)\n",
      "3. Keyboard control implementation (e.g. which keys to use, how to handle input)\n",
      "\n",
      "These sub-questions are derived from the conversation samples provided, where the assistant clarifies the areas that need clarification and then asks specific questions to the user.\n",
      "---\n",
      "Question: 1. What are the key architectural components of an LLM-powered autonomous agent system, such as the role of the LLM, perception, and action modules?\n",
      "Answer: Based on the provided background question-answer pairs and additional context, the key architectural components of an LLM-powered autonomous agent system are:\n",
      "\n",
      "1. **LLM (Large Language Model)**: Functions as the agent's core controller, responsible for planning, reflection, and refinement**. It breaks down large tasks into smaller, manageable subgoals, enables self-criticism and self-reflection over past actions, and learns from mistakes to refine future steps.\n",
      "\n",
      "2. **Planning Module**: Decomposes complex tasks into smaller, manageable subgoals using techniques such as Chain of Thought (CoT) and \"Tree of Thoughts\". This module enables the agent to plan ahead and utilize more test-time computation.\n",
      "\n",
      "3. **Perception Module**: Although not explicitly mentioned, it can be inferred that the perception module is responsible for gathering information from the environment, which is then used by the planning module to decompose tasks and make decisions.\n",
      "\n",
      "4. **Action Module**: Executes the planned subgoals, taking into account the agent's strengths and pursuing simple strategies with no legal complications. The action module may also involve interacting with external tools or expert modules, as seen in the MRKL architecture.\n",
      "\n",
      "5. **Memory Module**: Stores information about past actions, enabling the LLM to learn from mistakes and improve the quality of final results.\n",
      "\n",
      "6. **External Tools or Expert Modules**: These can be neural (e.g., deep learning models) or symbolic (e.g., math calculator, currency converter, weather API), and are used by the LLM to route inquiries to the best suitable expert module.\n",
      "\n",
      "These key architectural components work together to enable an LLM-powered autonomous agent system to efficiently handle complex tasks and make decisions independently without seeking user assistance.\n",
      "---\n",
      "Question: 2. What are the primary functions and responsibilities of the LLM within the autonomous agent system, including tasks such as reasoning, planning, and decision-making?\n",
      "Answer: Based on the provided background question-answer pairs and additional context, the primary functions and responsibilities of the LLM within the autonomous agent system are:\n",
      "\n",
      "1. **Planning**: The LLM breaks down large tasks into smaller, manageable subgoals using techniques such as Chain of Thought (CoT) and Tree of Thoughts. This enables the agent to plan ahead and utilize more test-time computation.\n",
      "\n",
      "2. **Reasoning**: The LLM enables self-criticism and self-reflection over past actions, learns from mistakes, and refines them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "3. **Decision-making**: The LLM makes decisions by decomposing complex tasks into smaller subgoals, planning ahead, and utilizing external tools or expert modules** when necessary. It also involves interacting with external tools or expert modules, as seen in the MRKL architecture.\n",
      "\n",
      "4. **Reflection and Refinement**: The LLM learns from past actions, enables self-criticism, and self-reflection, and refines future steps, thereby improving the quality of final results.\n",
      "\n",
      "In summary, the LLM functions as the agent's core controller, responsible for planning, reflection, and refinement. It breaks down large tasks into smaller, manageable subgoals, enables self-criticism and self-reflection over past actions, and learns from mistakes to refine future steps.\n",
      "---\n",
      "Question: 3. How does the autonomous agent system integrate and leverage external data sources, such as sensors, APIs, and knowledge graphs, to inform its decision-making and action-taking processes?\n",
      "Answer: Based on the provided background question-answer pairs and additional context, the autonomous agent system integrates and leverages external data sources, such as sensors, APIs, and knowledge graphs, through its Perception Module, which gathers information from the environment. This information is then used by the Planning Module to decompose tasks and make decisions.\n",
      "\n",
      "Additionally, the system may also utilize external tools or expert modules, such as neural (e.g., deep learning models) or symbolic (e.g., math calculator, currency converter, weather API) modules, to route inquiries to the best suitable expert module. These external data sources inform the agent's decision-making and action-taking processes by providing relevant and timely information.\n",
      "\n",
      "For instance, in the context of the MRKL system, the agent may use sensors to gather environmental information, which is then used by the Planning Module to plan ahead and make decisions. The system may also utilize knowledge graphs to retrieve relevant context and inform the agent's behavior.\n",
      "\n",
      "In summary, the autonomous agent system integrates and leverages external data sources through its Perception Module, and external tools or expert modules, to inform its decision-making and action-taking processes.\n",
      "---\n",
      "Question: These sub-questions address distinct aspects of the main question, allowing them to be answered independently while still providing a comprehensive understanding of the main components of an LLM-powered autonomous agent system.\n",
      "Answer: The main question is about the main components of an LLM-powered autonomous agent system. The three sub-questions that break down the complex input question are:\n",
      "\n",
      "1. What are the key architectural components of an LLM-powered autonomous agent system, such as the role of the LLM, perception, and action modules?\n",
      "2. What are the primary functions and responsibilities of the LLM within the autonomous agent system, including tasks such as reasoning, planning, and decision-making?\n",
      "3. How does the autonomous agent system integrate and leverage external data sources, such as sensors, APIs, and knowledge graphs, to inform its decision-making and action-taking processes?\n",
      "\n",
      "These sub-questions address distinct aspects of the main question, allowing them to be answered independently while still providing a comprehensive understanding of the main components of an LLM-powered autonomous agent system.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format a question and answer pair as a string.\"\"\"\n",
    "    return f\"Question: {question}\\nAnswer: {answer}\\n\"\n",
    "\n",
    "# Initialize the language model (LLM)\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0)\n",
    "\n",
    "# Initialize Q&A pairs as an empty string\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "# Loop through the list of questions and process each one\n",
    "for q in questions:\n",
    "    # Define the RAG chain for retrieving and answering\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever,  # Retrieve context for the current question\n",
    "            \"question\": itemgetter(\"question\"),             # Extract the current question\n",
    "            \"q_a_pairs\": itemgetter(\"q_a_pairs\")            # Include existing Q&A pairs\n",
    "        }\n",
    "        | decomposition_prompt  # Apply the decomposition prompt\n",
    "        | llm                   # Use the LLM to generate an answer\n",
    "        | StrOutputParser()     # Parse the output into a clean string\n",
    "    )\n",
    "\n",
    "    # Generate an answer for the current question\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "\n",
    "    # Format the question and answer pair\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "\n",
    "    # Append the new Q&A pair to the existing pairs\n",
    "    q_a_pairs += f\"\\n---\\n{q_a_pair.strip()}\"\n",
    "\n",
    "# Print the final Q&A pairs (optional)\n",
    "print(q_a_pairs.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer Individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Pull the RAG prompt template from the hub\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    \"\"\"\n",
    "    Perform RAG (Retrieve and Generate) for each sub-question.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The main input question.\n",
    "        prompt_rag: The RAG prompt template.\n",
    "        sub_question_generator_chain: A chain that generates sub-questions.\n",
    "        retriever: A retriever object to fetch relevant documents.\n",
    "        llm: The language model for answering questions.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A list of answers and the corresponding sub-questions.\n",
    "    \"\"\"\n",
    "    # Generate sub-questions from the input question\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\": question})\n",
    "\n",
    "    # Initialize a list to store answers for each sub-question\n",
    "    rag_results = []\n",
    "\n",
    "    # Process each sub-question\n",
    "    for sub_question in sub_questions:\n",
    "        # Retrieve documents relevant to the sub-question\n",
    "        retrieved_docs = retriever.invoke(sub_question)\n",
    "\n",
    "        # Generate an answer using the RAG prompt, LLM, and retrieved documents\n",
    "        answer = (\n",
    "            prompt_rag  # Apply the RAG prompt\n",
    "            | llm       # Pass it through the language model\n",
    "            | StrOutputParser()  # Parse the LLM output into a clean string\n",
    "        ).invoke({\"context\": retrieved_docs, \"question\": sub_question})\n",
    "\n",
    "        # Append the generated answer to the results list\n",
    "        rag_results.append(answer)\n",
    "\n",
    "    return rag_results, sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided Q&A pairs, here is a clear and concise answer to the question:\n",
      "\n",
      "The main components of an LLM-powered autonomous agent system include:\n",
      "\n",
      "1. The LLM (Large Language Model) as the agent's core controller, responsible for task decomposition, planning, and decision-making.\n",
      "2. Planning component, which involves subgoal decomposition, reflection, and refinement, enabling the agent to break down complex tasks into smaller, manageable subgoals.\n",
      "3. Perception module, which integrates and leverages external data sources, such as sensors, APIs, and knowledge graphs, through its retrieval model, to inform the agent's behavior.\n",
      "4. Action module, which takes action based on the decisions made by the LLM.\n",
      "5. Reflection mechanism, which synthesizes memories into higher-level inferences over time and guides the agent's future behavior.\n",
      "\n",
      "These components work together to enable the autonomous agent system to make informed decisions and take actions in a complex environment.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"\n",
    "    Format a list of questions and their corresponding answers into a structured string.\n",
    "\n",
    "    Args:\n",
    "        questions (list): List of questions.\n",
    "        answers (list): List of answers corresponding to the questions.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted string of Q&A pairs.\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Question {i}: {q}\\nAnswer {i}: {a}\" for i, (q, a) in enumerate(zip(questions, answers), start=1)\n",
    "    ).strip()\n",
    "\n",
    "# Format the Q&A pairs to create the context\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Define the prompt template for synthesizing an answer\n",
    "template = \"\"\"\n",
    "Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize a clear and concise answer to the following question:\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define the RAG chain\n",
    "final_rag_chain = (\n",
    "    prompt  # Use the prompt template\n",
    "    | llm  # Pass the input through the language model\n",
    "    | StrOutputParser()  # Parse the output into a clean string\n",
    ")\n",
    "\n",
    "# Invoke the chain with the given context and question\n",
    "result = final_rag_chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "# Output the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 8 - STEP BACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# Define few-shot examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"What can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel was born in what country?\",\n",
    "        \"output\": \"What is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Transform examples into example messages using a message prompt template\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a few-shot prompt template with the example prompt and examples\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Combine the system message, few-shot examples, and user question into the final prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert in world knowledge. Your task is to reframe a specific question into a more generic, step-back question that is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Add few-shot examples\n",
    "        few_shot_prompt,\n",
    "        # Add the user question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do artificial intelligence systems break down complex tasks?'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | ChatGroq(model=\"llama3-70b-8192\", temperature=0) | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition for LLM (Large Language Model) agents refers to the process of breaking down complex tasks into smaller, manageable sub-tasks or steps. This is a crucial component of planning in LLM-powered autonomous agent systems.\n",
      "\n",
      "Task decomposition can be achieved through various methods, including:\n",
      "\n",
      "1. **Chain of Thought (CoT)**: A standard prompting technique that instructs the model to \"think step by step\" to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks, providing insight into the model's thinking process**.\n",
      "3. **Tree of Thoughts (ToT)**: An extension of CoT that explores multiple reasoning possibilities at each step, creating a tree structure. The search process can be done using Breadth-First Search (BFS) or Depth-First Search (DFS) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "4. **Simple prompting**: Using simple prompts like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\" to decompose tasks.\n",
      "5. **Task-specific instructions**: Using instructions like \"Write a story outline.\" for writing a novel.\n",
      "6. **Human inputs**: Decomposing tasks with human inputs.\n",
      "\n",
      "Task decomposition is essential in LLM-powered autonomous agent systems, as it enables the agent to plan ahead, understand the task's requirements, and execute the task efficiently.\n"
     ]
    }
   ],
   "source": [
    "# Define the response prompt template\n",
    "response_prompt_template = \"\"\"\n",
    "You are an expert in world knowledge. I will ask you a question. Your response should be comprehensive and aligned with the provided contexts if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# Normal Context:\n",
    "{normal_context}\n",
    "\n",
    "# Step-Back Context:\n",
    "{step_back_context}\n",
    "\n",
    "# Original Question:\n",
    "{question}\n",
    "\n",
    "# Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create the response prompt\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "# Define the chain for context retrieval and response generation\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve normal context using the original question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve step-back context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass through the original question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt  # Generate a prompt with the retrieved context and question\n",
    "    | ChatGroq(model=\"llama3-70b-8192\", temperature=0)  # Generate the response using the LLM\n",
    "    | StrOutputParser()  # Parse the response into a clean string\n",
    ")\n",
    "\n",
    "# Invoke the chain with the input question\n",
    "result = chain.invoke({\"question\": question})\n",
    "\n",
    "# Print the generated response\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 9 - HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a scientific paper passage that answers the question:\n",
      "\n",
      "**Task Decomposition for Large Language Models (LLMs) Agents**\n",
      "\n",
      "In the realm of artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools for tackling complex tasks, such as natural language processing, text generation, and dialogue systems. However, as the complexity of these tasks continues to grow, it has become increasingly important to develop strategies for decomposing them into more manageable sub-tasks. This process, known as task decomposition, is a crucial step in enabling LLM agents to reason about and execute complex tasks in a more efficient and effective manner.\n",
      "\n",
      "In the context of LLM agents, task decomposition involves identifying the constituent sub-tasks, or \"micro-tasks,\" that comprise a larger task. These micro-tasks are typically smaller, more focused, and more easily executable by the LLM agent. For example, in a task such as \"write a short story about a character who learns to play the guitar,\" the micro-tasks might include generating a character profile, creating a plot outline, and writing descriptive paragraphs about the character's guitar-playing experiences. By breaking down the task into these smaller components, the LLM agent can more easily reason about the task, allocate resources, and execute each micro-task in a more focused and efficient way.\n",
      "\n",
      "Task decomposition for LLM agents offers several benefits, including improved task execution efficiency, enhanced reasoning capabilities, and increased flexibility in adapting to changing task requirements. By decomposing complex tasks into smaller, more manageable components, LLM agents can better allocate their computational resources, reduce the risk of errors, and improve overall task performance. As the complexity of tasks continues to grow, the development of effective task decomposition strategies will play a critical role in unlocking the full potential of LLM agents.\n",
      "\n",
      "I hope this passage meets your requirements!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Define the HyDE document generation template\n",
    "template = \"\"\"\n",
    "Please write a scientific paper passage to answer the following question:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Passage:\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt template using ChatPromptTemplate\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define the document generation chain\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde  # Use the HyDE prompt template\n",
    "    | ChatGroq(model=\"llama3-70b-8192\", temperature=0)  # Generate the document using the language model\n",
    "    | StrOutputParser()  # Parse the LLM output into a clean string\n",
    ")\n",
    "\n",
    "# Define the question\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "# Invoke the chain to generate the passage\n",
    "generated_passage = generate_docs_for_retrieval.invoke({\"question\": question})\n",
    "\n",
    "# Print the generated passage\n",
    "print(generated_passage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='fcd3f123-3ba8-4a25-a748-1d5d82d3a970', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(id='67fd865c-e9d3-4a9e-b622-885a86a21611', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.'),\n",
       " Document(id='e7ffc73d-46f0-47b3-9127-aa96858fe71f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n",
       " Document(id='3311c858-038f-4855-935b-6f6219df6bb4', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Or\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retrieved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, task decomposition for LLM agents is the process of breaking down complex tasks into smaller, manageable subgoals. This can be done in three ways:\n",
      "\n",
      "1. By using simple prompting, such as \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\"\n",
      "2. By using task-specific instructions, such as \"Write a story outline.\" for writing a novel.\n",
      "3. With human inputs.\n",
      "\n",
      "Additionally, techniques like Chain of Thought (CoT) and Tree of Thoughts can be used to enhance task decomposition. CoT involves instructing the model to \"think step by step\" to decompose hard tasks into smaller and simpler steps. Tree of Thoughts extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Define the RAG prompt template\n",
    "template = \"\"\"\n",
    "Answer the following question based on the provided context:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt from the template\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define the RAG chain\n",
    "final_rag_chain = (\n",
    "    prompt  # Generate the prompt using the template\n",
    "    | ChatGroq(model=\"llama3-70b-8192\", temperature=0)  # Use the LLM to process the prompt\n",
    "    | StrOutputParser()  # Parse the output into a clean string\n",
    ")\n",
    "\n",
    "# Invoke the RAG chain with the context & question\n",
    "result = final_rag_chain.invoke({\n",
    "    \"context\": retrieved_docs,  # The retrieved documents as context\n",
    "    \"question\": question  # The question to be answered\n",
    "})\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MP-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
