{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'cortex'\n",
    "\n",
    "# Get keys from the environment\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if langchain_api_key:\n",
    "    os.environ['LANGCHAIN_API_KEY'] = langchain_api_key\n",
    "else:\n",
    "    raise ValueError(\"LANGCHAIN_API_KEY is not set in the environment.\")\n",
    "\n",
    "if groq_api_key:\n",
    "    os.environ['GROQ_API_KEY'] = groq_api_key\n",
    "else:\n",
    "    raise ValueError(\"GROQ_API_KEY is not set in the environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 5 - MULTI QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Ensure the USER_AGENT environment variable is set before any HTTP requests\n",
    "os.environ['USER_AGENT'] = 'your_custom_user_agent_string'  # Replace with your actual user agent\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split - Chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Embed\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "# Initialize HuggingFaceEmbeddings (updated)\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Index with FAISS\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=hf_embeddings)\n",
    "\n",
    "# Retrieve\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Query Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. How does LangChain enhance natural language processing tasks?\n",
      "2. Can you list the advantages of implementing LangChain in NLP projects?\n",
      "3. In what ways does LangChain improve the efficiency of NLP tasks?\n",
      "4. What are the key benefits of using LangChain for handling natural language processing tasks?\n",
      "5. How can LangChain contribute to the success of NLP projects?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define the template for generating multiple perspectives on the user's question\n",
    "template = \"\"\"\n",
    "You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "\n",
    "Provide these alternative questions separated by newlines.\n",
    "\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template using ChatPromptTemplate\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define the pipeline for generating alternative questions\n",
    "generate_queries = (\n",
    "    prompt_perspectives  # Use the prompt template\n",
    "    | ChatGroq(temperature=0)  # Use the ChatGroq model with zero temperature (deterministic responses)\n",
    "    | StrOutputParser()  # Parse the output into a clean format (a single string with questions separated by newlines)\n",
    "    | (lambda x: \"\\n\".join([q.strip() for q in x.split(\"\\n\") if q.strip()]))  # Clean and join questions with newlines\n",
    ")\n",
    "\n",
    "# Example: Use the pipeline to generate alternative questions for a given user question\n",
    "user_question = \"What are the benefits of using LangChain for NLP tasks?\"\n",
    "alternative_questions = generate_queries.invoke({\"question\": user_question})\n",
    "\n",
    "# Print the generated alternative questions\n",
    "print(alternative_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition for LLM agents refers to the process of breaking down a complex task into smaller, manageable subtasks that can be executed by a large language model (LLM) agent. This allows the agent to handle complex requests more effectively by addressing each subtask sequentially. The resulting subtasks can then be parsed, prioritized, and distributed to appropriate expert models for efficient processing. The order of tasks is important to consider due to logical relationships between them. If the user input cannot be parsed, the agent should return an empty JSON.\n"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG template\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGroq(temperature=0)\n",
    "\n",
    "# Define the final RAG chain\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")}  # Ensure the mapping is correct\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: \"\\n\".join([q.strip() for q in x.split(\"\\n\") if q.strip()]))\n",
    ")\n",
    "\n",
    "# Define the question\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "# Invoke the chain with the question and get the result\n",
    "result = final_rag_chain.invoke({\"question\": question})\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MP-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
